{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12365361,"sourceType":"datasetVersion","datasetId":7796348},{"sourceId":12594048,"sourceType":"datasetVersion","datasetId":7954533},{"sourceId":12623069,"sourceType":"datasetVersion","datasetId":7975528},{"sourceId":12650172,"sourceType":"datasetVersion","datasetId":7994394}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats         # for zscore\nfrom scipy.stats import ttest_ind\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nimport warnings\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Suppress just the catastrophic cancellation warnings from scipy\nwarnings.filterwarnings(\n    \"ignore\"\n)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-02T15:30:35.064604Z","iopub.execute_input":"2025-08-02T15:30:35.064822Z","iopub.status.idle":"2025-08-02T15:30:44.637675Z","shell.execute_reply.started":"2025-08-02T15:30:35.064795Z","shell.execute_reply":"2025-08-02T15:30:44.637095Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/sbi-data/HACKATHON_TRAINING_DATA/HACKATHON_TRAINING_DATA.CSV')\ndf_test = pd.read_csv('/kaggle/input/sbi-data/HACKATHON_TRAINING_DATA/HACKATHON_PREDICTION_DATA.CSV')","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:30:44.639092Z","iopub.execute_input":"2025-08-02T15:30:44.639582Z","iopub.status.idle":"2025-08-02T15:30:58.132797Z","shell.execute_reply.started":"2025-08-02T15:30:44.639562Z","shell.execute_reply":"2025-08-02T15:30:58.131968Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X = df_train.drop(columns=['TARGET'])\ny = df_train['TARGET']\n\nX_train, X_hold, y_train, y_hold = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:30:58.133559Z","iopub.execute_input":"2025-08-02T15:30:58.133784Z","iopub.status.idle":"2025-08-02T15:30:58.764585Z","shell.execute_reply.started":"2025-08-02T15:30:58.133766Z","shell.execute_reply":"2025-08-02T15:30:58.763719Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# === 2) Identify MCAR columns via t-test ===\nnum_cols = X_train.select_dtypes(include='number').columns\n\ndef is_mcar(df, col, alpha=0.05):\n    mask = df[col].isnull()\n    p_vals = []\n    for feat in num_cols.drop(col, errors='ignore'):\n        grp1 = df.loc[mask, feat].dropna()\n        grp2 = df.loc[~mask, feat].dropna()\n        if len(grp1) < 10 or len(grp2) < 10:\n            continue\n        _, p = ttest_ind(grp1, grp2, equal_var=False)\n        p_vals.append(p)\n    if not p_vals:\n        return True\n    signif = sum(p < alpha for p in p_vals)\n    return (signif / len(p_vals)) < alpha\n\nresults = {}\nfor col in X_train.columns[X_train.isnull().any()]:\n    if col in num_cols:\n        results[col] = is_mcar(X_train, col)\n\nmcar_cols = [c for c,v in results.items() if v]\nmar_cols  = [c for c,v in results.items() if not v]\n\n# === 3) Define zero_fill & knn_cols from MAR columns ===\nzero_patterns = ['MNTH', 'NO_', 'PRI_', '_YR_', 'CRIFF', 'DEC_CRIFF']\nzero_fill = [c for c in mar_cols if any(p in c for p in zero_patterns)]\nknn_cols  = [c for c in mar_cols if c not in zero_fill]\ncat_cols = X_train.select_dtypes(include='object').columns.drop(['TARGET','UNIQUE_ID'], errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:30:58.765576Z","iopub.execute_input":"2025-08-02T15:30:58.765836Z","iopub.status.idle":"2025-08-02T15:32:40.286681Z","shell.execute_reply.started":"2025-08-02T15:30:58.765816Z","shell.execute_reply":"2025-08-02T15:32:40.285815Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# === 4) Preprocessor class ===\nclass Preprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, zero_fill_cols, mcar_cols, knn_cols,cat_cols,\n                 frac_def=0.15, outlier_contam=0.01, corr_thresh=0.90):\n        self.zero_fill_cols = zero_fill_cols\n        self.mcar_cols      = mcar_cols\n        self.knn_cols       = knn_cols\n        self.frac_def       = frac_def\n        self.outlier_contam = outlier_contam\n        self.corr_thresh    = corr_thresh\n        self.cat_cols       = cat_cols\n    def fit(self, df, y=None):\n        df0 = df.copy()\n        print(\"Starting to fit data\")\n        # 1) Imputation\n        df0[self.zero_fill_cols] = df0[self.zero_fill_cols].fillna(0)\n        self._si_mcar = SimpleImputer(strategy='median')\n        df0[self.mcar_cols] = self._si_mcar.fit_transform(df0[self.mcar_cols])\n        # self._knn_imputer = KNNImputer(n_neighbors=5)\n        # df0[self.knn_cols] = self._knn_imputer.fit_transform(df0[self.knn_cols])\n        self._si_knn = SimpleImputer(strategy='median')\n        df0[self.knn_cols] = self._si_knn.fit_transform(df0[self.knn_cols])\n        self.si_cat = SimpleImputer(strategy='most_frequent')\n        df0[self.cat_cols] = self.si_cat.fit_transform(df0[self.cat_cols])\n        print(\"Imputation done\")\n        # 2) Flag encoding & duration parsing\n        df0['SI_FLG']          = df0['SI_FLG'].map({'Y':1,'N':0})\n        df0['LOCKER_HLDR_IND'] = df0['LOCKER_HLDR_IND'].map({'Y':1,'N':0})\n        df0['UID_FLG']         = df0['UID_FLG'].map({'Y':1,'N':0})\n        df0['INB_FLG']         = df0['INB_FLG'].map({'Y':1,'N':0})\n        df0['EKYC_FLG']        = df0['EKYC_FLG'].map({'Y':1,'N':0})\n        df0['KYC_FLG']         = df0['KYC_FLG'].replace({'N':0,'1':1,'2':2,'Y':2}).astype(int)\n\n        def _parse(s):\n            y,m = s.split('yrs ')\n            return int(y)*12 + int(m.replace('mon',''))\n        df0['AVERAGE_ACCT_AGE1']      = df0['AVERAGE_ACCT_AGE1'].apply(_parse)\n        df0['CREDIT_HISTORY_LENGTH1'] = df0['CREDIT_HISTORY_LENGTH1'].apply(_parse)\n        print(\"column parsing for those with years_months done\")\n        # 3) One‑hot encode remaining object cols\n        cats = df0.select_dtypes(include='object').columns.tolist()\n        if 'TARGET' in cats: cats.remove('TARGET')\n\n        cats = df0.select_dtypes(include='object').columns.tolist()\n        # 1) build train-side dummies and save their columns\n        train_dummies = pd.get_dummies(df0[cats], drop_first=True, dtype=int)\n        self._dummy_cols = train_dummies.columns.tolist()\n        # 2) replace original cats with exactly those dummies\n        df0 = pd.concat([df0.drop(columns=cats), train_dummies], axis=1)\n        \n        print(\"One Hot encoding done\")\n        # 4) Drop UNIQUE_ID\n        df0 = df0.drop(columns=['UNIQUE_ID'], errors='ignore')\n\n        # 5) Winsorize numeric tails\n        num = [c for c in df0.select_dtypes(include=['int64','float64']).columns\n               if c!='TARGET']\n        self._winsor = {c: df0[c].quantile([0.01,0.99]).values for c in num}\n        for c,(lo,hi) in self._winsor.items():\n            df0[c] = df0[c].clip(lo,hi)\n        print(\"Winzorization done\")\n        # 6) Yeo–Johnson on skewed continuous\n        skew = df0[num].skew().abs().sort_values(ascending=False)\n        cont = [c for c in skew[skew>2].index if df0[c].nunique()>10]\n        self._pt = PowerTransformer(method='yeo-johnson', standardize=True)\n        trans = self._pt.fit_transform(df0[cont])\n        df0[cont] = np.clip(trans, -3, 3)\n        \n        print(\"Power Transformation of data done to reduce skewness\")\n        # 7) Drop highly correlated\n        corr = df0[num].corr().abs()\n        upper = corr.where(np.triu(np.ones(corr.shape),1).astype(bool))\n        self._corr_drop = [c for c in upper.columns if (upper[c]>self.corr_thresh).any()]\n        df0 = df0.drop(columns=self._corr_drop)\n\n        print(\"Dropped columns which were highly correlated\")\n        # 8) Outlier flagging on TARGET==0, full coverage but fast\n        nf = [c for c in df0.columns if df0[c].dtype in ['int64','float64'] and c!='TARGET']\n        df_neg = df0[df0['TARGET']==0]\n        \n        # --- A) z-score on full negatives (cheap) ---\n        z = np.abs(stats.zscore(df_neg[nf]))\n        mask_z = (z > 3).any(axis=1)\n        bad_z  = set(df_neg.index[mask_z])\n\n        print(\"zscore calculation done to identify outliers\")\n        \n        # --- B) IsolationForest trained on subsample, then predict on all ---\n        iso = IsolationForest(contamination=self.outlier_contam, random_state=42)\n        y_iso = iso.fit_predict(df_neg[nf])      # fit+predict on ~290k rows\n        bad_iso = set(df_neg.index[y_iso == -1])\n        print(\"Isolation forest calculation done to identify outliers\")\n        \n        # --- Combine: require both methods to flag (2-of-2) ---\n        drop0 = bad_z & bad_iso\n        \n        # --- Mix in a fraction of defaulters as before ---\n        pos_idx = df0[df0['TARGET']==1].index\n        n_def   = int(len(pos_idx)*self.frac_def)\n        mix_idx = np.random.RandomState(42).choice(pos_idx, size=n_def, replace=False)\n        \n        self.outlier_idx_ = set(drop0).union(mix_idx)\n        self.clean_idx_   = [i for i in df0.index if i not in self.outlier_idx_]\n        self.features_    = [c for c in df0.columns if c!='TARGET']\n        print(\"outliers cleaned\")\n        return self\n\n    def transform(self, df, return_outliers=False):\n        df0 = df.copy()\n\n        # repeated steps 1–7 using fitted attributes\n        df0[self.zero_fill_cols] = df0[self.zero_fill_cols].fillna(0)\n        df0[self.mcar_cols]      = self._si_mcar.transform(df0[self.mcar_cols])\n        # df0[self.knn_cols]       = self._knn_imputer.transform(df0[self.knn_cols])\n        df0[self.knn_cols]       = self._si_knn.transform(df0[self.knn_cols])\n        df0[self.cat_cols]       = self.si_cat.transform(df0[self.cat_cols])\n        \n        df0['SI_FLG']          = df0['SI_FLG'].map({'Y':1,'N':0})\n        df0['LOCKER_HLDR_IND'] = df0['LOCKER_HLDR_IND'].map({'Y':1,'N':0})\n        df0['UID_FLG']         = df0['UID_FLG'].map({'Y':1,'N':0})\n        df0['INB_FLG']         = df0['INB_FLG'].map({'Y':1,'N':0})\n        df0['EKYC_FLG']        = df0['EKYC_FLG'].map({'Y':1,'N':0})\n        df0['KYC_FLG']         = df0['KYC_FLG'].replace({'N':0,'1':1,'2':2,'Y':2}).astype(int)\n        df0['AVERAGE_ACCT_AGE1']      = df0['AVERAGE_ACCT_AGE1'].apply(lambda s: int(s.split('yrs ')[0])*12 + int(s.split('yrs ')[1].replace('mon','')))\n        df0['CREDIT_HISTORY_LENGTH1'] = df0['CREDIT_HISTORY_LENGTH1'].apply(lambda s: int(s.split('yrs ')[0])*12 + int(s.split('yrs ')[1].replace('mon','')))\n\n        cats = df0.select_dtypes(include='object').columns.tolist()\n        test_dummies = pd.get_dummies(df0[cats], drop_first=True, dtype=int)\n\n        # reindex to *exactly* the train-side columns, filling new/missing with 0\n        test_dummies = test_dummies.reindex(\n            columns=self._dummy_cols,\n            fill_value=0\n        )\n\n        # drop the raw object-cols & concat our aligned dummies\n        df0 = pd.concat(\n            [df0.drop(columns=cats), test_dummies],\n            axis=1\n        )\n\n        for c,(lo,hi) in self._winsor.items():\n            df0[c] = df0[c].clip(lo,hi)\n\n        cont = self._pt.feature_names_in_\n        df0[cont] = np.clip(self._pt.transform(df0[cont]), -3, 3)\n\n        df0 = df0.drop(columns=self._corr_drop, errors='ignore')\n\n        X = df0[self.features_]\n\n        if return_outliers and 'TARGET' in df0:\n            return df0.loc[self.clean_idx_], df0.loc[list(self.outlier_idx_)]\n        return X\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:32:40.287641Z","iopub.execute_input":"2025-08-02T15:32:40.287921Z","iopub.status.idle":"2025-08-02T15:32:40.310257Z","shell.execute_reply.started":"2025-08-02T15:32:40.287889Z","shell.execute_reply":"2025-08-02T15:32:40.309674Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# === 5) Fit & transform training data ===\nprep = Preprocessor(\n    zero_fill_cols=zero_fill,\n    mcar_cols=mcar_cols,\n    knn_cols=knn_cols,\n    cat_cols=cat_cols,\n    frac_def=0.15\n).fit(pd.concat([X_train, y_train], axis=1))","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:32:40.310971Z","iopub.execute_input":"2025-08-02T15:32:40.311696Z","iopub.status.idle":"2025-08-02T15:33:39.563683Z","shell.execute_reply.started":"2025-08-02T15:32:40.311669Z","shell.execute_reply":"2025-08-02T15:33:39.562857Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Starting to fit data\nImputation done\ncolumn parsing for those with years_months done\nOne Hot encoding done\nWinzorization done\nPower Transformation of data done to reduce skewness\nDropped columns which were highly correlated\nzscore calculation done to identify outliers\nIsolation forest calculation done to identify outliers\noutliers cleaned\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df_tr_clean, df_tr_outliers = prep.transform(pd.concat([X_train, y_train], axis=1), return_outliers=True)\nX_hold_prep = prep.transform(pd.concat([X_hold, y_hold], axis=1))\ny_tr_clean         = df_tr_clean['TARGET']\nX_tr_clean         = df_tr_clean.drop(columns=['TARGET'])\nX_hold_clean       = X_hold_prep.copy()\n\n# === 7) Summaries ===\nprint(\"Train clean shape:\", df_tr_clean.shape)\nprint(\"Train outliers shape:\", df_tr_outliers.shape)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:33:39.565800Z","iopub.execute_input":"2025-08-02T15:33:39.566062Z","iopub.status.idle":"2025-08-02T15:33:45.195703Z","shell.execute_reply.started":"2025-08-02T15:33:39.566044Z","shell.execute_reply":"2025-08-02T15:33:45.194815Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train clean shape: (255608, 109)\nTrain outliers shape: (6584, 109)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\n\nclass StackingEnsembleLR:\n    def __init__(self, n_folds=5, random_state=42):\n        self.n_folds = n_folds\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        X = X.values if hasattr(X, 'values') else np.array(X)\n        y = y.values if hasattr(y, 'values') else np.array(y)\n        ratio = np.sum(y == 0) / np.sum(y == 1)\n\n        self.base_models_ = [\n            RandomForestClassifier(class_weight='balanced',\n                                   n_estimators=200,\n                                   random_state=self.random_state,\n                                   n_jobs=-1),\n            lgb.LGBMClassifier(class_weight='balanced',\n                               n_estimators=200,\n                               random_state=self.random_state,\n                               verbose=-1),\n            xgb.XGBClassifier(scale_pos_weight=ratio,\n                              n_estimators=200,\n                              use_label_encoder=False,\n                              eval_metric='auc',\n                              verbosity=0,\n                              random_state=self.random_state)\n        ]\n        print(\"Base Models ready\")\n        n_models = len(self.base_models_)\n        meta_train = np.zeros((X.shape[0], n_models))\n        skf = StratifiedKFold(n_splits=self.n_folds,\n                              shuffle=True,\n                              random_state=self.random_state)\n        print(\"Starting out of fold predictions\")\n        for i, model in enumerate(self.base_models_):\n            oof = np.zeros(X.shape[0])\n            for tr_idx, val_idx in skf.split(X, y):\n                X_tr, y_tr = X[tr_idx], y[tr_idx]\n                pos_idx = np.where(y_tr == 1)[0]\n                neg_idx = np.where(y_tr == 0)[0]\n                np.random.seed(self.random_state)\n                neg_sample = np.random.choice(neg_idx, size=len(pos_idx), replace=False)\n                keep = np.concatenate([pos_idx, neg_sample])\n                model.fit(X_tr[keep], y_tr[keep])\n                oof[val_idx] = model.predict_proba(X[val_idx])[:, 1]\n            meta_train[:, i] = oof\n        print(\"DONE training Stratified predictions model\")\n\n        # Logistic Regression\n        self.meta_model_ = LogisticRegression(class_weight='balanced',\n                                              max_iter=1000,\n                                              random_state=self.random_state)\n        self.meta_model_.fit(meta_train, y)\n        print(\"TRAINED Logistic Regression\")\n        print(\"RETRAINING base models.....\")\n        for model in self.base_models_:\n            model.fit(X, y)\n        return self\n\n    def predict_proba(self, X):\n        X = X.values if hasattr(X, 'values') else np.array(X)\n        meta_test = np.column_stack([\n            model.predict_proba(X)[:, 1] for model in self.base_models_\n        ])\n        return self.meta_model_.predict_proba(meta_test)\n\n    def predict(self, X):\n        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:33:45.196801Z","iopub.execute_input":"2025-08-02T15:33:45.197098Z","iopub.status.idle":"2025-08-02T15:33:45.207638Z","shell.execute_reply.started":"2025-08-02T15:33:45.197070Z","shell.execute_reply":"2025-08-02T15:33:45.207068Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n\n# 0a) sanitize column names\nsafe_cols = {c: re.sub(r'[^0-9a-zA-Z_]', '_', c) for c in X_tr_clean.columns}\nX_tr_clean   = X_tr_clean.rename(columns=safe_cols)\nX_hold_clean = X_hold_clean.rename(columns=safe_cols)\ntrain_feats = X_tr_clean.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2025-08-02T15:33:45.208211Z","iopub.execute_input":"2025-08-02T15:33:45.208441Z","iopub.status.idle":"2025-08-02T15:33:45.402502Z","shell.execute_reply.started":"2025-08-02T15:33:45.208425Z","shell.execute_reply":"2025-08-02T15:33:45.401883Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = StackingEnsembleLR(n_folds=5, random_state=42)\nmodel.fit(X_tr_clean, y_tr_clean)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:33:45.403286Z","iopub.execute_input":"2025-08-02T15:33:45.403491Z","iopub.status.idle":"2025-08-02T15:38:16.499578Z","shell.execute_reply.started":"2025-08-02T15:33:45.403474Z","shell.execute_reply":"2025-08-02T15:38:16.498967Z"}},"outputs":[{"name":"stdout","text":"Base Models ready\nStarting out of fold predictions\nDONE training Stratified predictions model\nTRAINED Logistic Regression\nRETRAINING base models.....\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<__main__.StackingEnsembleLR at 0x7ce8ad5fba90>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"X_hold_clean = X_hold_clean.reindex(columns=train_feats, fill_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:38:16.500821Z","iopub.execute_input":"2025-08-02T15:38:16.501034Z","iopub.status.idle":"2025-08-02T15:38:16.520566Z","shell.execute_reply.started":"2025-08-02T15:38:16.501017Z","shell.execute_reply":"2025-08-02T15:38:16.519734Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\nprobs = model.predict_proba(X_hold_clean)[:,1]\npreds = model.predict(X_hold_clean)\n\nprint(\"AUC:\", roc_auc_score(y_hold, probs))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_hold, preds))\nprint(\"Classification report:\\n\", classification_report(y_hold, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:38:16.521508Z","iopub.execute_input":"2025-08-02T15:38:16.521775Z","iopub.status.idle":"2025-08-02T15:38:20.730879Z","shell.execute_reply.started":"2025-08-02T15:38:16.521751Z","shell.execute_reply":"2025-08-02T15:38:20.730267Z"}},"outputs":[{"name":"stdout","text":"AUC: 0.921400485662665\nConfusion matrix:\n [[52884  5577]\n [ 1784  5304]]\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.97      0.90      0.93     58461\n           1       0.49      0.75      0.59      7088\n\n    accuracy                           0.89     65549\n   macro avg       0.73      0.83      0.76     65549\nweighted avg       0.92      0.89      0.90     65549\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\ndf_test = pd.read_csv('/kaggle/input/sbi-data/HACKATHON_TRAINING_DATA/HACKATHON_PREDICTION_DATA.CSV')\ndf_test_work = df_test.copy()\nX_test_clean = prep.transform(df_test_work)\nX_test_clean = X_test_clean.rename(columns=safe_cols)\nX_test_clean = X_test_clean.reindex(columns=train_feats, fill_value=0)\nX_test_clean = X_test_clean.fillna(0)\ndf_test['pred'] = model.predict(X_test_clean)\n\ndf_test.to_csv('sbi_test_with_preds.csv', index=False)\ndf_test.to_parquet('sbi_test_with_preds.parquet', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:48:39.532312Z","iopub.execute_input":"2025-08-02T15:48:39.533067Z","iopub.status.idle":"2025-08-02T15:49:09.937164Z","shell.execute_reply.started":"2025-08-02T15:48:39.533042Z","shell.execute_reply":"2025-08-02T15:49:09.936532Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:49:24.388209Z","iopub.execute_input":"2025-08-02T15:49:24.388523Z","iopub.status.idle":"2025-08-02T15:49:24.417932Z","shell.execute_reply.started":"2025-08-02T15:49:24.388503Z","shell.execute_reply":"2025-08-02T15:49:24.417367Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   ACCT_AGE     LIMIT       OUTS  ACCT_RESIDUAL_TENURE  LOAN_TENURE  \\\n0     2.694  729200.0  541543.71                 3.308         2192   \n1     5.652  980500.0  426219.82                 2.349         2922   \n2     5.737  980500.0  413595.82                 2.265         2922   \n3     6.479  735500.0  221620.79                 1.607         2953   \n4     6.394  735500.0  231762.79                 1.692         2953   \n\n   INSTALAMT SI_FLG     AGE  VINTAGE  KYC_SCR  ... CREDIT_HISTORY_LENGTH1  \\\n0    15247.0      Y  38.915   18.765    110.0  ...             4yrs 11mon   \n1    15836.0      Y  51.436   15.665    110.0  ...              5yrs 6mon   \n2    15836.0      Y  51.521   15.750    110.0  ...              5yrs 6mon   \n3    11996.0      Y  33.526   14.702    198.0  ...              6yrs 4mon   \n4    11996.0      Y  33.441   14.617    198.0  ...              6yrs 4mon   \n\n  NO_OF_INQUIRIES1 INCOME_BAND1           AGREG_GROUP   PRODUCT_TYPE  \\\n0              2.0            E  #Total Xpress Credit  PERSONAL LOAN   \n1              0.0            F      #Total Auto Loan      AUTO LOAN   \n2              0.0            F      #Total Auto Loan      AUTO LOAN   \n3              0.0            F      #Total Auto Loan      AUTO LOAN   \n4              0.0            F      #Total Auto Loan      AUTO LOAN   \n\n   LATEST_CR_DAYS  LATEST_DR_DAYS  TIME_PERIOD  UNIQUE_ID  pred  \n0            10.0           45715        FEB25       2202     0  \n1            25.0           45715        FEB25       2209     0  \n2            28.0           45746        MAR25       2211     0  \n3             4.0           45746        MAR25       2217     0  \n4             0.0           45715        FEB25       2218     0  \n\n[5 rows x 139 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ACCT_AGE</th>\n      <th>LIMIT</th>\n      <th>OUTS</th>\n      <th>ACCT_RESIDUAL_TENURE</th>\n      <th>LOAN_TENURE</th>\n      <th>INSTALAMT</th>\n      <th>SI_FLG</th>\n      <th>AGE</th>\n      <th>VINTAGE</th>\n      <th>KYC_SCR</th>\n      <th>...</th>\n      <th>CREDIT_HISTORY_LENGTH1</th>\n      <th>NO_OF_INQUIRIES1</th>\n      <th>INCOME_BAND1</th>\n      <th>AGREG_GROUP</th>\n      <th>PRODUCT_TYPE</th>\n      <th>LATEST_CR_DAYS</th>\n      <th>LATEST_DR_DAYS</th>\n      <th>TIME_PERIOD</th>\n      <th>UNIQUE_ID</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.694</td>\n      <td>729200.0</td>\n      <td>541543.71</td>\n      <td>3.308</td>\n      <td>2192</td>\n      <td>15247.0</td>\n      <td>Y</td>\n      <td>38.915</td>\n      <td>18.765</td>\n      <td>110.0</td>\n      <td>...</td>\n      <td>4yrs 11mon</td>\n      <td>2.0</td>\n      <td>E</td>\n      <td>#Total Xpress Credit</td>\n      <td>PERSONAL LOAN</td>\n      <td>10.0</td>\n      <td>45715</td>\n      <td>FEB25</td>\n      <td>2202</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.652</td>\n      <td>980500.0</td>\n      <td>426219.82</td>\n      <td>2.349</td>\n      <td>2922</td>\n      <td>15836.0</td>\n      <td>Y</td>\n      <td>51.436</td>\n      <td>15.665</td>\n      <td>110.0</td>\n      <td>...</td>\n      <td>5yrs 6mon</td>\n      <td>0.0</td>\n      <td>F</td>\n      <td>#Total Auto Loan</td>\n      <td>AUTO LOAN</td>\n      <td>25.0</td>\n      <td>45715</td>\n      <td>FEB25</td>\n      <td>2209</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.737</td>\n      <td>980500.0</td>\n      <td>413595.82</td>\n      <td>2.265</td>\n      <td>2922</td>\n      <td>15836.0</td>\n      <td>Y</td>\n      <td>51.521</td>\n      <td>15.750</td>\n      <td>110.0</td>\n      <td>...</td>\n      <td>5yrs 6mon</td>\n      <td>0.0</td>\n      <td>F</td>\n      <td>#Total Auto Loan</td>\n      <td>AUTO LOAN</td>\n      <td>28.0</td>\n      <td>45746</td>\n      <td>MAR25</td>\n      <td>2211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.479</td>\n      <td>735500.0</td>\n      <td>221620.79</td>\n      <td>1.607</td>\n      <td>2953</td>\n      <td>11996.0</td>\n      <td>Y</td>\n      <td>33.526</td>\n      <td>14.702</td>\n      <td>198.0</td>\n      <td>...</td>\n      <td>6yrs 4mon</td>\n      <td>0.0</td>\n      <td>F</td>\n      <td>#Total Auto Loan</td>\n      <td>AUTO LOAN</td>\n      <td>4.0</td>\n      <td>45746</td>\n      <td>MAR25</td>\n      <td>2217</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.394</td>\n      <td>735500.0</td>\n      <td>231762.79</td>\n      <td>1.692</td>\n      <td>2953</td>\n      <td>11996.0</td>\n      <td>Y</td>\n      <td>33.441</td>\n      <td>14.617</td>\n      <td>198.0</td>\n      <td>...</td>\n      <td>6yrs 4mon</td>\n      <td>0.0</td>\n      <td>F</td>\n      <td>#Total Auto Loan</td>\n      <td>AUTO LOAN</td>\n      <td>0.0</td>\n      <td>45715</td>\n      <td>FEB25</td>\n      <td>2218</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 139 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df_test['pred'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T15:49:48.918781Z","iopub.execute_input":"2025-08-02T15:49:48.919045Z","iopub.status.idle":"2025-08-02T15:49:48.940541Z","shell.execute_reply.started":"2025-08-02T15:49:48.919025Z","shell.execute_reply":"2025-08-02T15:49:48.939786Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"pred\n0    167031\n1     24662\nName: count, dtype: int64"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}